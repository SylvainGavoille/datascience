{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3c5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130989d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63863d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_path = os.path.join(cwd,'dac_sample.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96263cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar_file = tf.keras.utils.get_file(tar_path, 'https://labs.criteo.com/wp-content/uploads/2015/04/dac_sample.tar.gz')\n",
    "tar_file = tf.keras.utils.get_file(tar_path, 'http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1179b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "my_tar = tarfile.open(tar_file)\n",
    "my_tar.extractall(cwd) # specify which folder to extract to\n",
    "my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc0c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mdac_sample.tar.gz\u001b[0m\r\n",
      "dac_sample.txt\r\n",
      "\u001b[01;32mdeep-learning-recommendation-model-dlrm.ipynb\u001b[0m*\r\n",
      "deep-learning-recommendation-model-xDeepFM.ipynb\r\n",
      "deep_CTR.ipynb\r\n",
      "dlrm.ipynb\r\n",
      "license.txt\r\n",
      "readme.txt\r\n",
      "test_tf.ipynb\r\n",
      "test_xdeepFM-new_dataset.ipynb\r\n",
      "test_xdeepFM.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c47573",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(cwd, \"dac_sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac14971",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['label', *(f'I{i}' for i in range(1, 14)), *(f'C{i}' for i in range(1, 27))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca6cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 14:57:39.496530: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2022-02-09 14:57:39.496567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-I6J6GOV): /proc/driver/nvidia/version does not exist\n",
      "2022-02-09 14:57:39.496857: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "records_type =[tf.int32, *(tf.constant([0.0], dtype=tf.float32) for _ in range(1,14)), *(tf.constant([\"missing\"], dtype=tf.string) for _ in range (1,27))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7df46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.CsvDataset(file_path, field_delim=\"\\t\", record_defaults=records_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c70f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = sum(1 for _ in ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec6901d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601cdab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced1c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [*(f'I{i}' for i in range(1, 14)), *(f'C{i}' for i in range(1, 27))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef516d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 10\n",
    "NUM_FEATURES0 = len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78345f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_csv_row(*vals):\n",
    "    real_feats = vals[1:15]\n",
    "    cat_feats = vals[15:]\n",
    "    class_label = vals[0]\n",
    "    features = dict(zip(columns, real_feats+cat_feats))\n",
    "    return features, class_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d57791",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.map(_parse_csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fbf8fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'C1': <tf.Tensor: shape=(), dtype=string, numpy=b'68fd1e64'>,\n",
      "   'C10': <tf.Tensor: shape=(), dtype=string, numpy=b'a8cd5504'>,\n",
      "   'C11': <tf.Tensor: shape=(), dtype=string, numpy=b'b2cb9c98'>,\n",
      "   'C12': <tf.Tensor: shape=(), dtype=string, numpy=b'37c9c164'>,\n",
      "   'C13': <tf.Tensor: shape=(), dtype=string, numpy=b'2824a5f6'>,\n",
      "   'C14': <tf.Tensor: shape=(), dtype=string, numpy=b'1adce6ef'>,\n",
      "   'C15': <tf.Tensor: shape=(), dtype=string, numpy=b'8ba8b39a'>,\n",
      "   'C16': <tf.Tensor: shape=(), dtype=string, numpy=b'891b62e7'>,\n",
      "   'C17': <tf.Tensor: shape=(), dtype=string, numpy=b'e5ba7672'>,\n",
      "   'C18': <tf.Tensor: shape=(), dtype=string, numpy=b'f54016b9'>,\n",
      "   'C19': <tf.Tensor: shape=(), dtype=string, numpy=b'21ddcdc9'>,\n",
      "   'C2': <tf.Tensor: shape=(), dtype=string, numpy=b'80e26c9b'>,\n",
      "   'C20': <tf.Tensor: shape=(), dtype=string, numpy=b'b1252a9d'>,\n",
      "   'C21': <tf.Tensor: shape=(), dtype=string, numpy=b'07b5194c'>,\n",
      "   'C22': <tf.Tensor: shape=(), dtype=string, numpy=b'missing'>,\n",
      "   'C23': <tf.Tensor: shape=(), dtype=string, numpy=b'3a171ecb'>,\n",
      "   'C24': <tf.Tensor: shape=(), dtype=string, numpy=b'c5c50484'>,\n",
      "   'C25': <tf.Tensor: shape=(), dtype=string, numpy=b'e8b83407'>,\n",
      "   'C26': <tf.Tensor: shape=(), dtype=string, numpy=b'9727dd16'>,\n",
      "   'C3': <tf.Tensor: shape=(), dtype=string, numpy=b'fb936136'>,\n",
      "   'C4': <tf.Tensor: shape=(), dtype=string, numpy=b'7b4723c4'>,\n",
      "   'C5': <tf.Tensor: shape=(), dtype=string, numpy=b'25c83c98'>,\n",
      "   'C6': <tf.Tensor: shape=(), dtype=string, numpy=b'7e0ccccf'>,\n",
      "   'C7': <tf.Tensor: shape=(), dtype=string, numpy=b'de7995b8'>,\n",
      "   'C8': <tf.Tensor: shape=(), dtype=string, numpy=b'1f89b562'>,\n",
      "   'C9': <tf.Tensor: shape=(), dtype=string, numpy=b'a73ee510'>,\n",
      "   'I1': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
      "   'I10': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
      "   'I11': <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
      "   'I12': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
      "   'I13': <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
      "   'I2': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
      "   'I3': <tf.Tensor: shape=(), dtype=float32, numpy=5.0>,\n",
      "   'I4': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
      "   'I5': <tf.Tensor: shape=(), dtype=float32, numpy=1382.0>,\n",
      "   'I6': <tf.Tensor: shape=(), dtype=float32, numpy=4.0>,\n",
      "   'I7': <tf.Tensor: shape=(), dtype=float32, numpy=15.0>,\n",
      "   'I8': <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
      "   'I9': <tf.Tensor: shape=(), dtype=float32, numpy=181.0>},\n",
      "  <tf.Tensor: shape=(), dtype=int32, numpy=0>)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5bf7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, ds_test = get_dataset_partitions_tf(dataset, ds_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02844914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def _get_bins(ds, col):\n",
    "    ds_tmp = ds.batch(128)\n",
    "    quantiles = [ x/10.0 for x in range(11)]\n",
    "    y = []\n",
    "    for x in ds_tmp:\n",
    "        y.extend(list(x[0][col].numpy())) \n",
    "    return np.quantile(y, quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bc72382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_unique(ds, col):\n",
    "    ds_tmp = ds.batch(128)\n",
    "    uniques = set()\n",
    "    for x in ds_tmp:\n",
    "        u,i = tf.unique(x[0][col])\n",
    "        for v in u.numpy():\n",
    "            uniques.add(v)\n",
    "    return list(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2692a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for i in range(1, 14):\n",
    "    column_name = f'I{i}'\n",
    "    bins = list(np.unique(_get_bins(ds_train, column_name)))\n",
    "    tmp_num = tf.feature_column.numeric_column(column_name)\n",
    "    tmp_num_buck = tf.feature_column.bucketized_column(tmp_num, boundaries=bins)\n",
    "    tmp_cat_embedding = tf.feature_column.embedding_column(tmp_num_buck, dimension=EMBEDDING_SIZE)\n",
    "    feature_columns.append(tmp_cat_embedding)\n",
    "for i in range(1, 27):\n",
    "    column_name = f'C{i}'\n",
    "    unique_values = _get_unique(ds_train, column_name)\n",
    "    tmp_cat = tf.feature_column.categorical_column_with_vocabulary_list(column_name, unique_values)\n",
    "    tmp_cat_embedding = tf.feature_column.embedding_column(tmp_cat, dimension=EMBEDDING_SIZE)\n",
    "    feature_columns.append(tmp_cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86edb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6110dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.shuffle(1024).batch(128)\n",
    "ds_valid = ds_valid.shuffle(1024).batch(128)\n",
    "ds_test = ds_test.shuffle(1024).batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99251597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CINLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, num_features0, num_featuresk_1, num_featuresk):\n",
    "        super(CINLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size # d\n",
    "        self.num_features0 = num_features0 # m\n",
    "        self.num_featuresk_1 = num_featuresk_1 # Hk-1\n",
    "        self.num_featuresk = num_featuresk # Hk\n",
    "        self.w = self.add_weight(shape=[self.num_featuresk, \n",
    "                                         self.num_featuresk_1, \n",
    "                                         self.num_features0])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs_0 = inputs[:,:self.num_features0*self.embedding_size]\n",
    "        inputs_k_1 = inputs[:,self.num_features0*self.embedding_size:(self.num_features0+self.num_featuresk_1)*self.embedding_size]\n",
    "        pooling = inputs[:,(self.num_features0+self.num_featuresk_1)*self.embedding_size:]\n",
    "        inputs_0_reshape = tf.reshape(inputs_0, shape=[-1, self.num_features0, self.embedding_size])\n",
    "        inputs_k_1_reshape = tf.reshape(inputs_k_1, shape=[-1, self.num_featuresk_1, self.embedding_size])\n",
    "        hadamard_result = tf.einsum('ife,ige->iefg', inputs_k_1_reshape, inputs_0_reshape)\n",
    "        outputs_reshape = tf.einsum('hfg,iefg->ihe', self.w, hadamard_result)\n",
    "        pooling_k = tf.reduce_sum(outputs_reshape, 2)\n",
    "        pooling_concatenate = tf.concat([pooling, pooling_k], axis=1)\n",
    "        outputs = tf.reshape(outputs_reshape, shape= [-1, self.num_featuresk*self.embedding_size])\n",
    "        return tf.concat([inputs_0, outputs, pooling_concatenate], axis=1)\n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30f14e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(input_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(input_size,)))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=None))\n",
    "    return model\n",
    "    \n",
    "def CINmodel(embedding_size, num_features0, arch_CIN):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for m_k_1,m_k in zip(arch_CIN[:-1], arch_CIN[1:]):\n",
    "#         print(f\"m_k_1  {m_k_1}, m_k {m_k}\")\n",
    "        model.add(CINLayer(embedding_size, num_features0, m_k_1, m_k))\n",
    "    return model\n",
    "\n",
    "def DNNmodel(input_size, arch_DNN, activation='relu'):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(input_size,)))\n",
    "    for units in arch_DNN:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=activation))\n",
    "    return model\n",
    "\n",
    "class xDeepFM(tf.keras.Model):\n",
    "    def __init__(self, embedding_size, num_features0, feature_layer, arch_CIN, arch_DNN):\n",
    "        super(xDeepFM, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_features0 = num_features0\n",
    "        self.feature_layer = feature_layer\n",
    "        self.num_feature_last_CIN = arch_CIN[-1]\n",
    "        self.model_CIN = CINmodel(embedding_size, num_features0, arch_CIN)\n",
    "        self.model_DNN = DNNmodel(embedding_size*num_features0, arch_DNN)\n",
    "        self.model_linear = Linear(embedding_size*num_features0+sum(arch_CIN)+arch_DNN[-1])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        embeddings = self.feature_layer(inputs)\n",
    "        inputs_0_reshape = tf.reshape(embeddings, shape=[-1, self.num_features0, self.embedding_size])\n",
    "        pooling_0 = tf.reduce_sum(inputs_0_reshape, 2)\n",
    "        concat_input_CIN = tf.concat([embeddings, embeddings, pooling_0], axis=1)\n",
    "        concat_output_CIN = self.model_CIN(concat_input_CIN)\n",
    "        poolings = concat_output_CIN[:,(self.num_features0+self.num_feature_last_CIN)*self.embedding_size:]\n",
    "        outs_DNN = self.model_DNN(embeddings)\n",
    "        inputs_linear = tf.concat([embeddings, poolings, outs_DNN], axis=1)\n",
    "        return self.model_linear(inputs_linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94d69917",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_CIN=[39,50,50]\n",
    "arch_DNN=[512,256,128]\n",
    "\n",
    "model = xDeepFM(EMBEDDING_SIZE, NUM_FEATURES0, feature_layer, arch_CIN, arch_DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df2bc1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "625/625 [==============================] - 27s 35ms/step - loss: 0.4667 - accuracy: 0.7858 - val_loss: 0.4485 - val_accuracy: 0.7904\n",
      "Epoch 2/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.4140 - accuracy: 0.8073 - val_loss: 0.4217 - val_accuracy: 0.8082\n",
      "Epoch 3/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.3215 - accuracy: 0.8558 - val_loss: 0.4019 - val_accuracy: 0.8194\n",
      "Epoch 4/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.2042 - accuracy: 0.9149 - val_loss: 0.3949 - val_accuracy: 0.8345\n",
      "Epoch 5/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.1213 - accuracy: 0.9529 - val_loss: 0.4001 - val_accuracy: 0.8427\n",
      "Epoch 6/100\n",
      "625/625 [==============================] - 23s 36ms/step - loss: 0.0746 - accuracy: 0.9723 - val_loss: 0.4076 - val_accuracy: 0.8540\n",
      "Epoch 7/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.0518 - accuracy: 0.9819 - val_loss: 0.4227 - val_accuracy: 0.8587\n",
      "Epoch 8/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.0400 - accuracy: 0.9863 - val_loss: 0.4738 - val_accuracy: 0.8619\n",
      "Epoch 9/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.0306 - accuracy: 0.9903 - val_loss: 0.5096 - val_accuracy: 0.8666\n",
      "Epoch 10/100\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.0252 - accuracy: 0.9925 - val_loss: 0.5387 - val_accuracy: 0.8626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c7c577a30>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds_train,\n",
    "          validation_data=ds_valid,\n",
    "          callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "            ],\n",
    "        epochs=100,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "046728d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 4s 10ms/step - loss: 0.4596 - accuracy: 0.8126\n",
      "Accuracy 0.8126000165939331\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f817031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"x_deep_fm_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_features (DenseFeatur  multiple                 2028930   \n",
      " es)                                                             \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 1029)              173550    \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 128)               364416    \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 1)                 658       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,567,554\n",
      "Trainable params: 2,567,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b6398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
